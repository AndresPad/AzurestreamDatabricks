{"cells":[{"cell_type":"markdown","source":["# <img src ='https://airsblobstorage.blob.core.windows.net/airstream/Asset 256.png' width=\"50px\"> Stream data into Azure Databricks using Event Hubs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fbc4a69-1e1a-47e3-b98e-944a7f13c313"}}},{"cell_type":"code","source":["%scala\n\nClass.forName(\"org.apache.spark.sql.eventhubs.EventHubsSource\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ed5ac6d-29a9-41a4-9c5b-7d2592fb3850"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Namespace Connection String\nnamespaceConnectionString = \"Endpoint=sb://YOUREVENTHUB.servicebus.windows.net/;SharedAccessKeyName=YOUREVENTHUBACCESSPOLICY;SharedAccessKey=YOURACCESSKEY\"\n\n# Event Hub Name\neventHubName = \"databricks-eh\"\n\n# Event Hub Connection String\neventHubConnectionString = namespaceConnectionString + \";EntityPath=\" + eventHubName\n\n# Event Hub Configuration\neventHubConfiguration = {\n  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(eventHubConnectionString)  \n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"270649f1-92ea-4364-9f01-72a34e5d995c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n\nevent_hub_name = \"databricks-eh\"\nconnection_string = dbutils.secrets.get(scope=\"akv-bck-scope\", key=\"EventHubCnx-Azustream\") + \";EntityPath=\" + event_hub_name\n\nehConf = {\n  'eventhubs.connectionString' : connection_string\n}\n  \nprint(\"Event Hub Connection String From Azure Key Vault: {}\".format(connection_string))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd1f37cd-749b-4e27-8f30-904f02bf7fd2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a Streaming DataFrame\n# Read directly from Event Hub or Iot Hub using the EventHubs library for Databricks\ninputDF = (\n  spark.readStream.format(\"eventhubs\")                                               # Read from IoT Hubs directly\n    .options(**eventHubConfiguration)                                                # Use the Event-Hub-enabled connect string\n    .load()                                                                          # Load the data\n)\n\n# Read directly from Event Hub or Iot Hub using the EventHubs library for Databricks\neventhubstreamDF = (\n  spark.readStream.format(\"eventhubs\")                                               # Read from IoT Hubs directly\n    .options(**ehConf)                                                               # Use the Event-Hub-enabled connect string\n    .load()                                                                          # Load the data\n)\n\n# Schema must be provided for a streaming data frame\n# Some of the data sources like Event Hubs provide the schema out of the box"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53f13433-1e51-45d5-b30a-c90be66f8e15"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Check to see of the Data Frame is a Streaming Data Frame\ninputDF.isStreaming\n\neventhubstreamDF.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58b09914-46e2-420d-981d-8dd1f77c347e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Add the sink to a Memor Sink for Debugging\n# Remember theire are two sinks available for debugging - Memory Sink and Console Sink\nstreamingMemoryQuery = (\n                          inputDF\n                              .writeStream\n                              .queryName(\"MemoryQuery\")\n                              .format(\"memory\") \n                              .trigger(processingTime = '10 seconds')\n                              .start()\n                       )\n\n# Show users the Raw Data and the Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a61ef29-2184-4807-8b9e-5f6dc4563806"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#streamingMemoryQuery.lastProgress"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b7e6221-469c-4d0d-8de0-3b3e1efe019b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Lets now check and see whats in the inputDF stream\n#You'll be able to see the raw data that's in the Event Hub\ndisplay(\n      inputDF,\n      streamName = \"DisplayMemoryQuery\",\n      processingTime = '10 seconds'  \n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48b05ad1-7554-4f53-a345-8f4bc5c8ee68"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n#Let's transform the body to get the actual data and display the actual json that is coming from Event Hub\nrawDF = (\n            inputDF\n                .withColumn(\n                              \"rawdata\",\n                              col(\"body\").cast(\"string\")\n                           )  \n                .select(\"rawdata\")\n        )\n\ndisplay(\n      rawDF,\n      streamName = \"DisplayMemoryQuery\",\n      processingTime = '10 seconds'  \n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a26115c2-5e22-4c8b-ae91-47bd6e371b09"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n#Now to extract the actual dat from the raw JSON data string let's define the schema\nschema = (\n            StructType()\n               .add(\"Id\", \"integer\")\n               .add(\"VendorId\", \"integer\")\n               .add(\"PickupTime\", \"timestamp\")\n               .add(\"CabLicense\", \"string\")\n               .add(\"DriverLicense\", \"string\")\n               .add(\"PickupLocationId\", \"integer\")\n               .add(\"PassengerCount\", \"integer\")\n               .add(\"RateCodeId\", \"integer\")\n         )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a709aabe-eb18-4e7c-936a-235c0733966f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rawDF = (\n            rawDF\n                .select(\n                          from_json(\n                                      col(\"rawdata\"),\n                                      schema\n                                   )\n                          .alias(\"taxidata\")\n                       )                        \n                .select(\n                          \"taxidata.Id\",\n                          \"taxidata.VendorId\",\n                          \"taxidata.PickupTime\",\n                          \"taxidata.CabLicense\",\n                          \"taxidata.DriverLicense\",\n                          \"taxidata.PickupLocationId\",\n                          \"taxidata.PassengerCount\",\n                          \"taxidata.RateCodeId\",\n                       )\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7183184-5646-443e-9214-544e1cf81a37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transformedDF = (\n                    rawDF\n                        .withColumn(\"TripType\",\n                                        when(\n                                                col(\"RateCodeId\") == \"6\",\n                                                    \"SharedTrip\"\n                                            )\n                                        .otherwise(\"SoloTrip\")\n                                   )  \n                        .drop(\"RateCodeId\")\n                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b48c9c6-99b6-4758-a3d1-8a6fadc655a8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transformedDF = (\n                    transformedDF\n                        .where(\"PassengerCount > 0\")\n                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7766e72-02f2-403c-a7ad-053e55120459"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(\n      transformedDF,\n      streamName = \"DisplayMemoryQuery\",\n      processingTime = '10 seconds'  \n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e102d8a1-005e-455d-b2f8-a5a9ad7f592a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n#Lets Chain Everything Together\n\ntransformedDF = (\n                    spark\n                        .readStream\n                        .format(\"eventhubs\")                      #1. Read messages from Event Hub\n                        .options(**eventHubConfiguration)\n                        .load()\n  \n                        .withColumn(                              #2. Convert raw binary body data to a string\n                                      \"rawdata\",\n                                      col(\"body\").cast(\"string\")\n                                   )\n  \n                        .select(\n                                  from_json(                      #3. Transform the string into JSON data and give it an alias\n                                              col(\"rawdata\"),\n                                              schema\n                                           )\n                                  .alias(\"taxidata\")\n                               )\n  \n                        .select(                                  #4. Extract the data and create separate columns from the JSON\n                                  \"taxidata.Id\",\n                                  \"taxidata.VendorId\",\n                                  \"taxidata.PickupTime\",\n                                  \"taxidata.CabLicense\",\n                                  \"taxidata.DriverLicense\",\n                                  \"taxidata.PickupLocationId\",\n                                  \"taxidata.PassengerCount\",\n                                  \"taxidata.RateCodeId\",\n                               ) \n  \n                        .withColumn(\"TripType\",                  #5. Add Transformations: Derive a new column & Drop a column\n                                        when(\n                                                col(\"RateCodeId\") == \"6\",\n                                                    \"SharedTrip\"\n                                            )\n                                        .otherwise(\"SoloTrip\")\n                                   )  \n                        .drop(\"RateCodeId\")\n\n                        .where(\"PassengerCount > 0\")             #6. Filter out any data\n                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25a0d95c-7b46-443b-ae8b-fc5ad3bf578b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Now instead of writing streaming data to CSV let's write to Parquet\n#Writing to Parquet is slower than CSV but Querying is much faster than CSV/JSON\nrawStreamingFileQuery = (\n                            rawDF                             \n                                .writeStream\n                                .queryName(\"RawTaxiQuery\")\n                                .format(\"csv\")\n                                .option(\"path\", \"/mnt/datalake/Raw/\")\n                                .option(\"checkpointLocation\", \"/mnt/datalake/checkpointRaw\")\n                                .trigger(processingTime = '10 seconds')                                \n                                .start()  \n                        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a8c067b-d68b-448f-a618-596fa445a96c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nprocessedStreamingFileQuery = (\n                                  transformedDF                             \n                                      .writeStream\n                                      .queryName(\"ProcessedTaxiQuery\")\n                                      .format(\"parquet\")\n                                      .option(\"path\", \"/mnt/datalake/Processed/\")\n                                      .option(\"checkpointLocation\", \"/mnt/datalake/checkpointProcessed\")\n                                      .trigger(processingTime = '3 seconds')\n                                      .start()  \n                              )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a37e2ce2-feed-42bf-b777-602d5ddc50bc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transformedDF.createOrReplaceTempView(\"ProcessedTaxiData\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec3f3215-cebc-4dd5-beaa-83c92dda9373"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sqlDF = spark.sql(\"SELECT PickupLocationId, COUNT(*) FROM ProcessedTaxiData GROUP BY PickupLocationId\")\n\ndisplay(sqlDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bd73ee4-a1b4-4ea7-ae6e-896f15047fb5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT PickupLocationId, COUNT(*)\nFROM ProcessedTaxiData\nGROUP BY PickupLocationId"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7208f30c-ef52-4c09-95ab-b6bb3bb62500"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["taxiZones = (\n                spark\n                    .read\n                    .option(\"header\", \"true\")\n                    .option(\"inferSchema\", \"true\")\n                    .csv(\"/mnt/datalake/StaticData/TaxiZones.csv\")\n            )\n\ndisplay(taxiZones)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bb773cc-ceb2-4009-9275-b2aca765befa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["taxiZones.createOrReplaceTempView(\"TaxiZones\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d9ca95b-bdc9-4e1d-938a-b59e2681de37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT z.Zone\n    , COUNT(*) AS TripCount\nFROM ProcessedTaxiData p\n  INNER JOIN TaxiZones z ON p.PickupLocationId = z.LocationID\nWHERE z.Borough = 'Manhattan'\nGROUP BY z.Zone"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97634644-e649-4410-b056-9d2d1ac9691d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.EventHubsStreamingPipeline","dashboards":[{"elements":[{"elementNUID":"97634644-e649-4410-b056-9d2d1ac9691d","guid":"5dc9b3dd-6eaa-49d5-8108-f46db691ae9c","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":11,"width":19,"z":null},"elementType":"command"}],"guid":"13fb4461-0ec0-4477-89a3-3c9baa4dfba4","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"d6b6b967-d2ea-4e5b-912e-5730da9aa790","origId":1936480217922972,"title":"Taxi Pickups","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1936480217922946}},"nbformat":4,"nbformat_minor":0}
