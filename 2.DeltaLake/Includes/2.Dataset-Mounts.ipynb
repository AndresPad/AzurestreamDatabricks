{"cells":[{"cell_type":"code","source":["\n%scala\nval tags = com.databricks.logging.AttributionContext.current.tags\n\n//*******************************************\n// GET VERSION OF APACHE SPARK\n//*******************************************\n\n// Get the version of spark\nval Array(sparkMajorVersion, sparkMinorVersion, _) = spark.version.split(\"\"\"\\.\"\"\")\n\n// Set the major and minor versions\nspark.conf.set(\"com.databricks.training.spark.major-version\", sparkMajorVersion)\nspark.conf.set(\"com.databricks.training.spark.minor-version\", sparkMinorVersion)\n\n//*******************************************\n// GET VERSION OF DATABRICKS RUNTIME\n//*******************************************\n\n// Get the version of the Databricks Runtime\nval version = {\n  val dbr = com.databricks.spark.util.SparkServerContext.serverVersion.replace(\"dbr-\", \"\")\n  val scalaMinMajVer = util.Properties.versionNumberString\n  val index = scalaMinMajVer.lastIndexOf(\".\")\n  val len = scalaMinMajVer.length\n  dbr + \".x-scala\" + scalaMinMajVer.dropRight(len - index)\n}\n\nval runtimeVersion = if (version != \"\") {\n  spark.conf.set(\"com.databricks.training.job\", \"false\")\n  version\n} else {\n  spark.conf.set(\"com.databricks.training.job\", \"true\")\n  dbutils.widgets.get(\"sparkVersion\")\n}\n\nval runtimeVersions = runtimeVersion.split(\"\"\"-\"\"\")\n// The GPU and ML runtimes push the number of elements out to 5\n// so we need to account for every scenario here. There should\n// never be a case in which there is less than two so we can fail\n// with an helpful error message for <2 or >5\nval (dbrVersion, scalaVersion) = {\n  runtimeVersions match {\n    case Array(d, _, _, _, s) => (d, s.replace(\"scala\", \"\"))\n    case Array(d, _, _, s)    => (d, s.replace(\"scala\", \"\"))\n    case Array(d, _, s)       => (d, s.replace(\"scala\", \"\"))\n    case Array(d, s)          => (d, s.replace(\"scala\", \"\"))\n    case _ =>\n      throw new IllegalArgumentException(s\"\"\"Dataset-Mounts: Cannot parse version(s) from \"${runtimeVersions.mkString(\", \")}\".\"\"\")\n  }\n}\nval Array(dbrMajorVersion, dbrMinorVersion, _*) = dbrVersion.split(\"\"\"\\.\"\"\")\n\n// Set the the major and minor versions\nspark.conf.set(\"com.databricks.training.dbr.version\", version)\nspark.conf.set(\"com.databricks.training.dbr.major-version\", dbrMajorVersion)\nspark.conf.set(\"com.databricks.training.dbr.minor-version\", dbrMinorVersion)\n\n//*******************************************\n// GET USERNAME AND USERHOME\n//*******************************************\n\n// Get the user's name\nval name = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nval username = if (name != \"unknown\") name else dbutils.widgets.get(\"databricksUsername\")\n\nval userhome = s\"dbfs:/user/$username\"\n\n// Set the user's name and home directory\nspark.conf.set(\"com.databricks.training.username\", username)\nspark.conf.set(\"com.databricks.training.userhome\", userhome)\n\n//**********************************\n// GET TAG VALUE\n// Find a given tag's value or return a supplied default value if not found\n//**********************************\n\ndef getTagValue(tagName: String, defaultValue: String = null): String = {\n  val tags = com.databricks.logging.AttributionContext.current.tags\n  val values = tags.collect({ case (t, v) if t.name == tagName => v }).toSeq\n  values.size match {\n    case 0 => defaultValue\n    case _ => values.head.toString\n  }\n}\n\n//**********************************\n// GET EXPERIMENT ID\n// JobId fallback in production mode\n//**********************************\n\ndef getExperimentId(): Long = {\n  val notebookId = getTagValue(\"notebookId\", null)\n  val jobId = getTagValue(\"jobId\", null)\n  \n  (notebookId != null) match { \n      case true => notebookId.toLong\n      case false => (jobId != null) match { \n        case true => jobId.toLong\n        case false => 0\n      }\n  }\n}\n\n\nspark.conf.set(\"com.databricks.training.experimentId\", getExperimentId())\n\n//**********************************\n// VARIOUS UTILITY FUNCTIONS\n//**********************************\n\ndef assertSparkVersion(expMajor:Int, expMinor:Int):String = {\n  val major = spark.conf.get(\"com.databricks.training.spark.major-version\")\n  val minor = spark.conf.get(\"com.databricks.training.spark.minor-version\")\n\n  if ((major.toInt < expMajor) || (major.toInt == expMajor && minor.toInt < expMinor)) {\n    throw new Exception(s\"This notebook must be ran on Spark version $expMajor.$expMinor or better, found Spark $major.$minor\")\n  }\n  return s\"$major.$minor\"\n}\n\ndef assertDbrVersion(expMajor:Int, expMinor:Int):String = {\n  val major = spark.conf.get(\"com.databricks.training.dbr.major-version\")\n  val minor = spark.conf.get(\"com.databricks.training.dbr.minor-version\")\n\n  if ((major.toInt < expMajor) || (major.toInt == expMajor && minor.toInt < expMinor)) {\n    throw new Exception(s\"This notebook must be ran on Databricks Runtime (DBR) version $expMajor.$expMinor or better, found $major.$minor.\")\n  }\n  return s\"$major.$minor\"\n}\n\ndef assertIsMlRuntime():Unit = {\n  if (version.contains(\"-ml-\") == false) {\n    throw new RuntimeException(s\"This notebook must be ran on a Databricks ML Runtime, found $version.\")\n  }\n}\n\n// **********************************\n//  GET AZURE DATASOURCE\n// **********************************\n\ndef getAzureDataSource(): (String,String,String) = {\n  val datasource = spark.conf.get(\"com.databricks.training.azure.datasource\").split(\"\\t\")\n  val source = datasource(0)\n  val sasEntity = datasource(1)\n  val sasToken = datasource(2)\n  return (source, sasEntity, sasToken)\n}\n\ndef initializeBrowserSideStats(): Unit = {\n  import java.net.URLEncoder.encode\n  import scala.collection.Map\n  import org.json4s.DefaultFormats\n  import org.json4s.jackson.JsonMethods._\n  import org.json4s.jackson.Serialization.write\n  import org.json4s.JsonDSL._\n\n  implicit val formats: DefaultFormats = DefaultFormats\n\n  val tags = com.databricks.logging.AttributionContext.current.tags\n\n  // Get the user's name and home directory\n  val username = spark.conf.get(\"com.databricks.training.username\", \"unknown-username\")\n  val userhome = spark.conf.get(\"com.databricks.training.userhome\", \"unknown-userhome\")\n  \n  val courseName = spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\")\n  val moduleName = spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\")\n\n  // Get the the major and minor versions\n  val dbrVersion = spark.conf.get(\"com.databricks.training.dbr.version\", \"0.0\")\n  val dbrMajorVersion = spark.conf.get(\"com.databricks.training.dbr.major-version\", \"0\")\n  val dbrMinorVersion = spark.conf.get(\"com.databricks.training.dbr.minor-version\", \"0\")\n\n  val sessionId = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_SESSION_ID, \"unknown-sessionId\")\n  val hostName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_HOST_NAME, \"unknown-host-name\")\n  val clusterMemory = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_CLUSTER_MEMORY, \"unknown-cluster-memory\")\n  val clientBranchName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_BRANCH_NAME, \"unknown-branch-name\")\n  val notebookLanguage = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_NOTEBOOK_LANGUAGE, \"unknown-notebook-language\")\n  val browserUserAgent = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER_AGENT, \"unknown-user-agent\")\n  val browserHostName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_HOST_NAME, \"unknown-host-name\")\n\n// Need to find docs or JAR file for com.databricks.logging.BaseTagDefinitions.TAG_ definitions  \n// Guessing TAG_BRANCH_NAME == clientBranchName\n//   val clientBranchName = (tags.filter(tup => tup._1.toString.contains(\"clientBranchName\")).map(tup => tup._2).head)\n// Guessing TAG_USER_AGENT == browserUserAgent\n//   val browserUserAgent = (tags.filter(tup => tup._1.toString.contains(\"browserUserAgent\")).map(tup => tup._2).head)\n// Guessing TAG_HOST_NAME == browserHostName\n//   val browserHostName = (tags.filter(tup => tup._1.toString.contains(\"browserHostName\")).map(tup => tup._2).head)\n\n// No TAG_ matches for these - wrap in try/catch if necessary\n  val sourceIpAddress = try { (tags.filter(tup => tup._1.toString.contains(\"sourceIpAddress\")).map(tup => tup._2).head) } catch { case e: Exception => \"unknown-source-ip\"}\n  val browserHash = try { (tags.filter(tup => tup._1.toString.contains(\"browserHash\")).map(tup => tup._2).head) } catch { case e: Exception => \"unknown-browser-hash\"}\n\n  val json = Map(\n    \"time\" -> java.time.Instant.now.toEpochMilli,\n    \"username\" -> username,\n    \"userhome\" -> userhome,\n    \"dbrVersion\" -> s\"$dbrMajorVersion.$dbrMinorVersion\",\n    \"tags\" -> tags.map(tup => (tup._1.name, tup._2))\n  )\n\n  val jsonString = write(json)\n  val tags_dump = write(tags.map(tup => (tup._1.name, tup._2)))\n  \n  val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n  \n  val html = s\"\"\"\n<html>\n<head>\n  <script src=\"https://files.training.databricks.com/static/js/classroom-support.min.js\"></script>\n  <script>\n<!--  \n    window.setTimeout( // Defer until bootstrap has enough time to async load\n      function(){ \n          Cookies.set(\"_academy_username\", \"$username\", {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_module_name\", \"$moduleName\", {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_course_name\", \"$courseName\", {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_sessionId\", \"$sessionId\", {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_hostName\", '$hostName', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_clusterMemory\", '$clusterMemory', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_clientBranchName\", '$clientBranchName', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_notebookLanguage\", '$notebookLanguage', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_sourceIpAddress\", '$sourceIpAddress', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_browserUserAgent\", '$browserUserAgent', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_browserHostName\", '$browserHostName', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_browserHash\", '$browserHash', {\"domain\":\".databricksusercontent.com\"});\n          Cookies.set(\"_academy_tags\", $jsonString, {\"domain\":\".databricksusercontent.com\"});\n      }, 2000\n    );\n-->    \n  </script>\n</head>\n<body>\n  Environment Setup Complete\n<script>\n</script>  \n</body>\n</html>\n\"\"\"\ndisplayHTML(html)\n  \n}\n\ndef showStudentSurvey():Unit = {\n  import java.net.URLEncoder.encode\n  val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n  val username = encode(spark.conf.get(\"com.databricks.training.username\", \"unknown-user\"), utf8)\n  val courseName = encode(spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\"), utf8)\n  val moduleNameUnencoded = spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\")\n  val moduleName = encode(moduleNameUnencoded, utf8)\n\n  import scala.collection.Map\n  import org.json4s.DefaultFormats\n  import org.json4s.jackson.JsonMethods._\n  import org.json4s.jackson.Serialization.write\n  import org.json4s.JsonDSL._\n\n  implicit val formats: DefaultFormats = DefaultFormats\n\n  val json = Map(\n    \"courseName\" -> courseName,\n    \"moduleName\" -> moduleName, // || \"unknown\",\n    \"name\" -> name,\n    \"time\" -> java.time.Instant.now.toEpochMilli,\n    \"username\" -> username,\n    \"userhome\" -> userhome,\n    \"dbrVersion\" -> s\"$dbrMajorVersion.$dbrMinorVersion\",\n    \"tags\" -> tags.map(tup => (tup._1.name, tup._2))\n  )\n\n  val jsonString = write(json)\n  val feedbackUrl = s\"https://engine-prod.databricks.training/feedback/$username/$courseName/$moduleName/\";\n  \n  val html = s\"\"\"\n  <html>\n  <head>\n    <script src=\"https://files.training.databricks.com/static/js/classroom-support.min.js\"></script>\n    <script>\n<!--    \n      window.setTimeout( // Defer until bootstrap has enough time to async load\n        () => { \n        //console.log($jsonString);\n        //console.log(JSON.stringify(\"$jsonString\");\n          var allCookies = Cookies.get();\n          Cookies.set(\"_academy_module_name\", \"$moduleName\", {\"domain\":\".databricksusercontent.com\"});\n\n          $$(\"#divComment\").css(\"display\", \"visible\");\n\n          // Emulate radio-button like feature for multiple_choice\n          $$(\".multiple_choicex\").on(\"click\", (evt) => {\n                const container = $$(evt.target).parent();\n                $$(\".multiple_choicex\").removeClass(\"checked\"); \n                $$(\".multiple_choicex\").removeClass(\"checkedRed\"); \n                $$(\".multiple_choicex\").removeClass(\"checkedGreen\"); \n                container.addClass(\"checked\"); \n                if (container.hasClass(\"thumbsDown\")) { \n                    container.addClass(\"checkedRed\"); \n                } else { \n                    container.addClass(\"checkedGreen\"); \n                };\n                \n                // Send the like/dislike before the comment is shown so we at least capture that.\n                // In analysis, always take the latest feedback for a module (if they give a comment, it will resend the like/dislike)\n                var json = { data: { liked: $$(\".multiple_choicex.checked\").attr(\"value\"), cookies: Cookies.get() } };\n                $$.ajax({\n                  type: 'PUT', \n                  url: '$feedbackUrl', \n                  data: JSON.stringify(json),\n                  dataType: 'json',\n                  processData: false\n                });\n                $$(\"#divComment\").show(\"fast\");\n          });\n\n\n           // Set click handler to do a PUT\n          $$(\"#btnSubmit\").on(\"click\", (evt) => {\n              // Use .attr(\"value\") instead of .val() - this is not a proper input box\n              var json = { data: { liked: $$(\".multiple_choicex.checked\").attr(\"value\"), comment: $$(\"#taComment\").val(), cookies: Cookies.get() } };\n\n              const msgThanks = \"Thank you for your feedback!\";\n              const msgError = \"There was an error submitting your feedback\";\n              const msgSending = \"Sending feedback...\";\n\n              $$(\"#feedback\").hide(\"fast\");\n              $$(\"#feedback-response\").html(msgSending);\n\n              $$.ajax({\n                type: 'PUT', \n                url: '$feedbackUrl', \n                data: JSON.stringify(json),\n                dataType: 'json',\n                processData: false\n              })\n                .done(function() {\n                  $$(\"#feedback-response\").html(msgThanks);\n                })\n                .fail(function() {\n                  $$(\"#feedback-response\").html(msgError);\n                })\n                ; // End of .ajax chain\n          });\n        }, 2000\n      );\n-->\n    </script>    \n    <style>\n.multiple_choicex > img:hover {    \n    background-color: white;\n    border-width: 0.15em;\n    border-radius: 5px;\n    border-style: solid;\n}\n.multiple_choicex.choice1 > img:hover {    \n    border-color: green;\n    background-color: green;\n}\n.multiple_choicex.choice2 > img:hover {    \n    border-color: red;\n    background-color: red;\n}\n.multiple_choicex {\n    margin: 1em;\n    padding: 0em;\n    background-color: white;\n    border: 0em;\n    border-style: solid;\n    border-color: green;\n}\n.multiple_choicex.checked {\n    border: 0.15em solid black;\n    background-color: white;\n    border-width: 0.5em;\n    border-radius: 5px;\n}\n.multiple_choicex.checkedGreen {\n    border-color: green;\n    background-color: green;\n}\n.multiple_choicex.checkedRed {\n    border-color: red;\n    background-color: red;\n}\n    </style>\n  </head>\n  <body>\n    <h2 style=\"font-size:28px; line-height:34.3px\"><img style=\"vertical-align:middle\" src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"/>What did you think?</h2>\n    <p>Please let us know how if you liked this module, <b>$moduleNameUnencoded</b></p>\n    <div id=\"feedback\" style=\"clear:both;display:table;\">\n      <span class=\"multiple_choicex choice1 thumbsUp\" value=\"true\"><img style=\"width:100px\" src=\"https://files.training.databricks.com/images/feedback/thumbs-up.png\"/></span>\n      <span class=\"multiple_choicex choice2 thumbsDown\" value=\"false\"><img style=\"width:100px\" src=\"https://files.training.databricks.com/images/feedback/thumbs-down.png\"/></span>\n      <span>\n        <div id=\"divComment\" style=\"display:none\">\n          <textarea id=\"taComment\" placeholder=\"How can we make this module better? (optional)\" style=\"margin:1em;width:100%;height:200px;display:block\"></textarea>\n          <button id=\"btnSubmit\">Send</button>\n        </div>\n      </span>\n    </div>\n    <div id=\"feedback-response\" style=\"color:green; margin-top: 1em\">&nbsp;</div>\n  </body>\n  </html>\n  \"\"\"\n  displayHTML(html);  \n}\n\nclass StudentsStatsService() extends org.apache.spark.scheduler.SparkListener {\n  import org.apache.spark.scheduler._\n\n  val hostname = \"engine-prod.databricks.training\"\n\n  def logEvent(eventType: String):Unit = {\n    import org.apache.http.entity._\n    import org.apache.http.impl.client.{HttpClients}\n    import org.apache.http.client.methods.HttpPost\n    import java.net.URLEncoder.encode\n    import org.json4s.jackson.Serialization\n    implicit val formats = org.json4s.DefaultFormats\n\n    var client:org.apache.http.impl.client.CloseableHttpClient = null\n\n    try {\n      val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n      val username = encode(spark.conf.get(\"com.databricks.training.username\", \"unknown-user\"), utf8)\n      val courseName = encode(spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\"), utf8)\n      val moduleName = encode(spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\"), utf8)\n      val event = encode(eventType, utf8)\n      val url = s\"https://$hostname/tracking/$courseName/$moduleName/$username/$event\"\n    \n      val content = Map(\n        \"tags\" -> tags.map(tup => (tup._1.name, s\"$tup._2\")),\n        \"courseName\" -> courseName, \n        \"moduleName\" -> moduleName,\n        \"username\" -> username,\n        \"eventType\" -> eventType,\n        \"eventTime\" -> s\"${System.currentTimeMillis}\"\n      )\n      \n      val output = Serialization.write(content)\n    \n      // Future: (clues from Brian) \n      // Threadpool - don't use defaultExecutionContext; create our own EC; EC needs to be in scope as an implicit (Future calls will pick it up)\n      // apply() on Future companion\n      // onSuccess(), onFailure() (get exception from failure); map over future, final future, onComplete() gives Try object (can )\n      //    Future {\n      val client = HttpClients.createDefault()\n      val httpPost = new HttpPost(url)\n      val entity = new StringEntity(Serialization.write(Map(\"data\" -> content)))      \n\n      httpPost.setEntity(entity)\n      httpPost.setHeader(\"Accept\", \"application/json\")\n      httpPost.setHeader(\"Content-type\", \"application/json\")\n\n      client.execute(httpPost)\n      \n    } catch {\n      case e:Exception => org.apache.log4j.Logger.getLogger(getClass).error(\"Databricks Academey stats service failure\", e)\n      \n    } finally {\n      if (client != null) {\n        try { client.close() } \n        catch { case _:Exception => () }\n      }\n    }\n  }\n  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = logEvent(\"JobEnd\" + jobEnd.jobId)\n  override def onJobStart(jobStart: SparkListenerJobStart): Unit = logEvent(\"JobStart: \" + jobStart.jobId)\n}\n\nval studentStatsService = new StudentsStatsService()\nif (spark.conf.get(\"com.databricks.training.studentStatsService.registered\", null) != \"registered\") {\n  sc.addSparkListener(studentStatsService)\n  spark.conf.set(\"com.databricks.training.studentStatsService\", \"registered\")\n}\nstudentStatsService.logEvent(\"Classroom-Setup\")\n  \n//*******************************************\n// CHECK FOR REQUIRED VERIONS OF SPARK & DBR\n//*******************************************\n\nassertDbrVersion(4, 0)\nassertSparkVersion(2, 3)\n\ndisplayHTML(\"<p>Initialized environment variables & functions...</p>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f5f622f-0d1a-4fd0-b79b-d5c127f45fa4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n\n#**********************************\n# VARIOUS UTILITY FUNCTIONS\n#**********************************\n\ndef assertSparkVersion(expMajor, expMinor):\n  major = spark.conf.get(\"com.databricks.training.spark.major-version\")\n  minor = spark.conf.get(\"com.databricks.training.spark.minor-version\")\n\n  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n    msg = \"This notebook must run on Spark version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n    raise Exception(msg)\n    \n  return major+\".\"+minor\n\ndef assertDbrVersion(expMajor, expMinor):\n  major = spark.conf.get(\"com.databricks.training.dbr.major-version\")\n  minor = spark.conf.get(\"com.databricks.training.dbr.minor-version\")\n\n  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n    msg = \"This notebook must run on Databricks Runtime (DBR) version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n    raise Exception(msg)\n    \n  return major+\".\"+minor\n\ndef assertIsMlRuntime():\n  version = spark.conf.get(\"com.databricks.training.dbr.version\")\n  if \"-ml-\" not in version:\n    raise Exception(\"This notebook must be ran on a Databricks ML Runtime, found {}.\".format(version))\n\n    \n#**********************************\n# GET AZURE DATASOURCE\n#**********************************\n\n\ndef getAzureDataSource(): \n  datasource = spark.conf.get(\"com.databricks.training.azure.datasource\").split(\"\\t\")\n  source = datasource[0]\n  sasEntity = datasource[1]\n  sasToken = datasource[2]\n  return (source, sasEntity, sasToken)\n\n    \n#**********************************\n# GET EXPERIMENT ID\n#**********************************\n\ndef getExperimentId():\n  return spark.conf.get(\"com.databricks.training.experimentId\")\n\n#**********************************\n# INIT VARIOUS VARIABLES\n#**********************************\n\nusername = spark.conf.get(\"com.databricks.training.username\", \"unknown-username\")\nuserhome = spark.conf.get(\"com.databricks.training.userhome\", \"unknown-userhome\")\n\nimport sys\npythonVersion = spark.conf.set(\"com.databricks.training.python-version\", sys.version[0:sys.version.index(\" \")])\n\nNone # suppress output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e27f2ff5-c25b-4464-8113-e96338d58b22"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n//**********************************\n// CREATE THE MOUNTS\n//**********************************\n\ndef getAwsRegion():String = {\n  try {\n    import scala.io.Source\n    import scala.util.parsing.json._\n\n    val jsonString = Source.fromURL(\"http://169.254.169.254/latest/dynamic/instance-identity/document\").mkString // reports ec2 info\n    val map = JSON.parseFull(jsonString).getOrElse(null).asInstanceOf[Map[Any,Any]]\n    map.getOrElse(\"region\", null).asInstanceOf[String]\n\n  } catch {\n    // We will use this later to know if we are Amazon vs Azure\n    case _: java.io.FileNotFoundException => null\n  }\n}\n\ndef getAzureRegion():String = {\n  import com.databricks.backend.common.util.Project\n  import com.databricks.conf.trusted.ProjectConf\n  import com.databricks.backend.daemon.driver.DriverConf\n\n  new DriverConf(ProjectConf.loadLocalConfig(Project.Driver)).region\n}\n\ndef mountFailed(msg:String): Unit = {\n  println(msg)\n}\n\ndef retryMount(source: String, mountPoint: String): Unit = {\n  try { \n    // Mount with IAM roles instead of keys for PVC\n    dbutils.fs.mount(source, mountPoint)\n  } catch {\n    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n  }\n}\n\ndef mount(source: String, extraConfigs:Map[String,String], mountPoint: String): Unit = {\n  try {\n    dbutils.fs.mount(source, mountPoint, extraConfigs=extraConfigs)\n  } catch {\n    case ioe: java.lang.IllegalArgumentException => retryMount(source, mountPoint)\n    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n  }\n}\n\ndef autoMount(fix:Boolean = false, failFast:Boolean = false, mountDir:String = \"/mnt/training\"): Unit = {\n  var awsRegion = getAwsRegion()\n\n  val (source, extraConfigs) = if (awsRegion != null)  {\n    spark.conf.set(\"com.databricks.training.region.name\", awsRegion)\n    getAwsMapping(awsRegion)\n\n  } else {\n    val azureRegion = getAzureRegion()\n    spark.conf.set(\"com.databricks.training.region.name\", azureRegion)\n    initAzureDataSource(azureRegion)\n  }\n  \n  val resultMsg = mountSource(fix, failFast, mountDir, source, extraConfigs)\n  displayHTML(\"<p>\" + resultMsg + \"</p>\")\n}\n\ndef initAzureDataSource(azureRegion:String):(String,Map[String,String]) = {\n  val mapping = getAzureMapping(azureRegion)\n  val (source, config) = mapping\n  val (sasEntity, sasToken) = config.head\n\n  val datasource = \"%s\\t%s\\t%s\".format(source, sasEntity, sasToken)\n  spark.conf.set(\"com.databricks.training.azure.datasource\", datasource)\n\n  return mapping\n}\n\ndef mountSource(fix:Boolean, failFast:Boolean, mountDir:String, source:String, extraConfigs:Map[String,String]): String = {\n  val mntSource = source.replace(awsAuth+\"@\", \"\")\n\n  if (dbutils.fs.mounts().map(_.mountPoint).contains(mountDir)) {\n    val mount = dbutils.fs.mounts().filter(_.mountPoint == mountDir).head\n    if (mount.source == mntSource) {\n      return s\"\"\"Datasets are already mounted to <b>$mountDir</b> from <b>$mntSource</b>\"\"\"\n      \n    } else if (failFast) {\n      throw new IllegalStateException(s\"Expected $mntSource but found ${mount.source}\")\n      \n    } else if (fix) {\n      println(s\"Unmounting existing datasets ($mountDir from $mntSource)\")\n      dbutils.fs.unmount(mountDir)\n      mountSource(fix, failFast, mountDir, source, extraConfigs)\n\n    } else {\n      return s\"\"\"<b style=\"color:red\">Invalid Mounts!</b></br>\n                      <ul>\n                      <li>The training datasets you are using are from an unexpected source</li>\n                      <li>Expected <b>$mntSource</b> but found <b>${mount.source}</b></li>\n                      <li>Failure to address this issue may result in significant performance degradation. To address this issue:</li>\n                      <ol>\n                        <li>Insert a new cell after this one</li>\n                        <li>In that new cell, run the command <code style=\"color:blue; font-weight:bold\">%scala fixMounts()</code></li>\n                        <li>Verify that the problem has been resolved.</li>\n                      </ol>\"\"\"\n    }\n  } else {\n    println(s\"\"\"Mounting datasets to $mountDir from $mntSource\"\"\")\n    mount(source, extraConfigs, mountDir)\n    return s\"\"\"Mounted datasets to <b>$mountDir</b> from <b>$mntSource<b>\"\"\"\n  }\n}\n\ndef fixMounts(): Unit = {\n  autoMount(true)\n}\n\nautoMount(true)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08292285-c7d3-4bf9-aa3f-80efc7ea7229"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.Dataset-Mounts","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1936480217922767}},"nbformat":4,"nbformat_minor":0}
