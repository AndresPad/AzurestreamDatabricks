{"cells":[{"cell_type":"markdown","source":["# <img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=80px> Delta Lake Architecture\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Discuss the advantages of Delta over a traditional Lambda architecture\n* Describe Bronze, Gold, and Silver tables\n* Unify batch jobs and streaming jobs using Delta Lake\n* Create a Delta Lake pipeline\n* Describe how Delta ensures ACID compliant transactions\n\nThis notebook demonstrates using Delta Lakes as an optimization layer on top of blob storage to ensure reliability (i.e. ACID compliance) and low latency within unified Streaming + Batch data pipelines.\n\nWe will discuss this relative to a traditional Lamdba architecture.\n\n<img src=https://files.training.databricks.com/images/adbcore/delta_azure.png width=800px>\n\nAn example of a Delta Lake Architecture might be the above.\n\n1. many **devices** generate data across different ingestion paths.\n2. streaming data can be ingested from **IOT Hub** or **Event Hub**\n3. batch data can be ingested by **Azure Data Factory** or **Databricks**\n4. Extracted, Transformed data is loaded into a **Delta Lake**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4677019b-9bdd-4397-a53e-093ecd3671fe"}}},{"cell_type":"markdown","source":["-sandbox\n## Lambda Architecture\n\nThe Lambda architecture is a big data processing architecture that combines both batch- and real-time processing methods.\nIt features an append-only immutable data source that serves as system of record. Timestamped events are appended to\nexisting events (nothing is overwritten). Data is implicitly ordered by time of arrival.\n\nNotice how there are really two pipelines here, one batch and one streaming, hence the name <i>lambda</i> architecture.\n\nIt is very difficult to combine processing of batch and real-time data as is evidenced by the diagram below.\n\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/lambda.png\" style=\"height: 400px\"/></div><br/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d69cc50-ca75-4627-a5b0-0939db8a1ec2"}}},{"cell_type":"markdown","source":["## Delta Lake Architecture\n\nThe Delta Lake Architecture is a vast improvmemt upon the traditional Lambda architecture. At each stage, we enrich our data through a unified pipeline that allows us to combine batch and streaming workflows through a shared filestore with ACID compliant transactions.\n\n**Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, etc.).\n\n**Silver** tables will provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n\n**Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n\nThe end outputs are actionable insights, dashboards and reports of business metrics.\n\n<img src=https://files.training.databricks.com/images/adbcore/delta/bronze_silver_gold.png width=800px>\n\n\nBy considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.\n\nEach stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bca032b-b52c-4c9b-b4e4-c471b9cd3333"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=\"80px\"/>\n\n# Unifying Structured Streaming with Batch Jobs with Delta Lake\n\nIn this notebook, we will explore combining streaming and batch processing with a single pipeline. We will begin by defining the following logic:\n\n- ingest streaming JSON data from disk and write it to a Delta Lake Table `/activity/Bronze`\n- perform a Stream-Static Join on the streamed data to add additional geographic data\n- transform and load the data, saving it out to our Delta Lake Table `/activity/Silver`\n- summarize the data through aggregation into the Delta Lake Table `/activity/Gold/groupedCounts`\n- materialize views of our gold table through streaming plots and static queries\n\nWe will then demonstrate that by writing batches of data back to our bronze table, we can trigger the same logic on newly loaded data and propagate our changes automatically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab0d904a-52b9-4089-9ffe-39774d0cacb7"}}},{"cell_type":"code","source":["%run \"./Includes/1.Environment-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bc504aa-fe82-4f29-8ee2-0c7e86604243"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Set shuffle partitions to match the number of executor cores in the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96f7fb81-d84f-45a2-94a5-900e375c6f76"}}},{"cell_type":"code","source":["%python\n\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0025946-af81-4e97-8f59-d8fb56090bce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n## Set up relevant Delta Lake paths\n\nThese paths will serve as the file locations for our Delta Lake tables.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Each streaming write has its own checkpoint directory.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> You cannot write out new Delta files within a repository that contains Delta files. Note that our hierarchy here isolates each Delta table into its own directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a42d3a9-3292-40a6-9d7e-aabb4384e57d"}}},{"cell_type":"code","source":["activityPath = userhome + \"/activity\"\n\nactivityBronzePath = activityPath + \"/Bronze\"\nactivityBronzeCheckpoint = activityBronzePath + \"/checkpoint\"\n\nactivitySilverPath = activityPath + \"/Silver\"\nactivitySilverCheckpoint = activitySilverPath + \"/checkpoint\"\n\nactivityGoldPath = activityPath + \"/Gold\"\ngroupedCountPath = activityGoldPath + \"/groupedCount\"\ngroupedCountCheckpoint = groupedCountPath + \"/checkpoint\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9aa948e-714a-4f25-8806-59beee76a002"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["displayHTML(\"<p>Activity Path: <b>\" + activityPath + \"</b></p>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ff7f92c-0d9b-41e6-9c89-9ad451ff200c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Reset Pipeline\n\nTo reset the pipeline, run the following:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16cecaf9-8dac-46d5-b8c3-791f754cd940"}}},{"cell_type":"code","source":["dbutils.fs.rm(activityPath, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc5b64ca-41ea-4038-897b-4053f6761421"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n## Datasets Used\nThis notebook will consume cell phone accelerometer data. Records have been downsampled so that the streaming data represents less than 3% of the total data being produced. The remainder will be processed as batches.\n\nThe following fields are present:\n\n- `Index`\n- `Arrival_Time`\n- `Creation_Time`\n- `x`\n- `y`\n- `z`\n- `User`\n- `Model`\n- `Device`\n- `gt`\n- `geolocation`\n\n## Define Schema\n\nFor streaming jobs, we need to define our schema before we start.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll reuse this same schema later in the notebook to define our batch processing, which will eliminate the jobs triggered by eliminating a file scan AND enforce the schema that we've defined here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbb43972-8400-4783-a653-3f01d4ea320d"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, LongType, StringType, DoubleType\n\nschema = StructType([\n  StructField(\"Arrival_Time\",LongType()),\n  StructField(\"Creation_Time\",LongType()),\n  StructField(\"Device\",StringType()),\n  StructField(\"Index\",LongType()),\n  StructField(\"Model\",StringType()),\n  StructField(\"User\",StringType()),\n  StructField(\"geolocation\",StructType([\n    StructField(\"city\",StringType()),\n    StructField(\"country\",StringType())\n  ])),\n  StructField(\"gt\",StringType()),\n  StructField(\"id\",LongType()),\n  StructField(\"x\",DoubleType()),\n  StructField(\"y\",DoubleType()),\n  StructField(\"z\",DoubleType())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2e6089c-6dfa-4a44-b4d1-e628e1c1ec20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n\n### Define Streaming Load from Files in Blob\n\nOur streaming source directory has 36 JSON files of 5k records each saved in a repository. Here, we'll trigger processing on files one at a time. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In a production setting, this same logic would allow us to only read new files written to our source directory. We could define `maxFilesPerTrigger` to control the amount of data we consume with each load, or omit this option to consume all new data on disk since the last time the stream has processed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6963d152-abea-4626-9595-386fd20cef49"}}},{"cell_type":"code","source":["rawEventsDF = (spark\n  .readStream\n  .format(\"json\")\n  .schema(schema)\n  .option(\"maxFilesPerTrigger\", 1)\n  .load(\"/mnt/training/definitive-guide/data/activity-json/streaming\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32563acd-133d-49ff-b1ca-2c0691d0328f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### WRITE Stream using Delta Lake\n\n#### General Notation\nUse this format to write a streaming job to a Delta Lake table.\n\n<pre>\n(myDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"append\")\n  .start(path)\n)\n</pre>\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> While we _can_ write directly to tables using the `.table()` notation, this will create fully managed tables by writing output to a default location on DBFS. This is not best practice for production jobs.\n\n#### Output Modes\nNotice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n* `append`: add only new records to output sink\n* `complete`: rewrite full output - applicable to aggregations operations\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> At present, `update` mode is **not** supported for streaming Delta jobs.\n\n#### Checkpointing\n\nWhen defining a Delta Lake streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n\n`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n\nThis is actually a structured streaming feature. It stores the current state of your streaming job.\n\nShould your streaming job stop for some reason and you restart it, it will continue from where it left off.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4c5d9b4-de02-4dd1-b161-8b3ae6ca195e"}}},{"cell_type":"code","source":["(rawEventsDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", activityBronzeCheckpoint)\n  .outputMode(\"append\")\n  .start(activityBronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebc548bc-edf8-47b4-b5ad-03ada7cc2e33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Load Static Lookup Table\n\nBefore enriching our bronze data, we will load a static lookup table for our country codes.\n\nHere, we'll use a parquet file that contains countries and their associated codes and abbreviations.\n\nWhile we can load this as a table (which will copy all files to the workspace and make it available to all users), here we'll manipulate it as a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67a01322-328c-4647-a17f-a99f77d55b00"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ngeoForLookupDF = (spark\n  .read\n  .format(\"parquet\")\n  .load(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet/\")\n  .select(col(\"EnglishShortName\").alias(\"country\"), col(\"alpha3Code\").alias(\"countryCode3\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee2261ca-db24-4ae0-8a37-cd95407f09aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n\n## Create QUERY tables (aka \"silver tables\")\n\nOur current bronze table contains nested fields, as well as time data that has been encoded in non-standard unix time (`Arrival_Time` is encoded as milliseconds from epoch, while `Creation_Time` records nanoseconds between record creation and receipt). \n\nWe also wish to enrich our data with 3 letter country codes for mapping purposes, which we'll obtain from a join with our `geoForLookupDF`.\n\nIn order to parse the data in human-readable form, we create query/silver tables out of the raw data.\n\nWe will stream from our previous file write, define transformations, and rewrite our data to disk.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not need to specify a schema when loading Delta files: it is inferred from the metadata!\n\nThe fields of a complex object can be referenced with a \"dot\" notation as in:\n\n`col(\"geolocation.country\")`\n\n\nA large number of these fields/columns can become unwieldy.\n\nFor that reason, it is common to extract the sub-fields and represent them as first-level columns as seen below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"006df97c-29f5-4320-b3e1-49b212b0e946"}}},{"cell_type":"code","source":["from pyspark.sql.functions import from_unixtime\n\nparsedEventsDF = (spark.readStream\n  .format(\"delta\")\n  .load(activityBronzePath)\n  .select(from_unixtime(col(\"Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"Device\"),\n          col(\"Index\"),\n          col(\"Model\"),\n          col(\"User\"),\n          col(\"gt\"),\n          col(\"x\"),\n          col(\"y\"),\n          col(\"z\"),\n          col(\"geolocation.country\").alias(\"country\"),\n          col(\"geolocation.city\").alias(\"city\"))\n  .join(geoForLookupDF, [\"country\"], \"left\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c13e5d2-d849-40e5-9fc0-670ba9b7c64f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Write to QUERY Tables (aka \"silver tables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52a16bfb-359d-4fb9-9cd4-067c0a9b7d50"}}},{"cell_type":"code","source":["(parsedEventsDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", activitySilverCheckpoint)\n  .outputMode(\"append\")\n  .start(activitySilverPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0e9f75e-aecd-4ee9-8b95-e42bf8fb37b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.read.text(\"dbfs:/user/anpadill@microsoft.com/activity/Bronze/_delta_log/00000000000000000000.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aca4dc7-a3bf-4b61-bf15-349a5e10318d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n#### See list of active streams.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You should currently see two active streams, one for each streaming write that you've triggered. If you have called `display` on either of your streaming DataFrames, you will see an additional stream, as `display` writes the stream to memory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d966f48f-4292-4ebc-83a0-c5a1b70382dd"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n  print(s.id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5648a8b6-d5db-44d1-a6eb-7bc4673f618b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Gold Table: Grouped Count of Events\n\nHere we read a stream of data from `activitySilverPath` and write another stream to `activityGoldPath/groupedCount`.\n\nThe data consists of a total counts of all event, grouped by `hour`, `gt`, and `countryCode3`.\n\nPerforming this aggregation allows us to reduce the total number of rows in our table from hundreds of thousands (or millions, once we've loaded our batch data) to dozens.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice that we're writing to a named directory within our gold path. If we wish to define additional aggregations, we would organize these parallel to thie directory to avoid metadata write conflicts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11704d82-ebee-4568-86a9-892edde26be0"}}},{"cell_type":"code","source":["from pyspark.sql.functions import window, hour\n\n(spark.readStream\n  .format(\"delta\")\n  .load(activitySilverPath)\n  .groupBy(window(\"Arrival_Time\", \"60 minute\"),\"gt\", \"countryCode3\")\n  .count()\n  .withColumn(\"hour\",hour(col(\"window.start\")))\n  .drop(\"window\")\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", groupedCountCheckpoint)\n  .outputMode(\"complete\")\n  .start(groupedCountPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c63cb0f2-b4ca-494c-863e-c79e56c5f6d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### CREATE A Table Using Delta Lake\n\nCreate a table called `gt_count` using `DELTA` out of the above data.\n\nNOTE: You will not be able to run this command until the `activityCountsQuery` has initialized."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"695edd1b-3196-446d-8223-df03fc7476b2"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  DROP TABLE IF EXISTS grouped_count\n\"\"\")\nspark.sql(\"\"\"\n  CREATE TABLE grouped_count\n  USING DELTA\n  LOCATION '{}'\n\"\"\".format(groupedCountPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ebe1bff-8167-4de7-8588-30a305a8150b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n#### Important Considerations for `complete` Output with Delta\n\nWhen using `complete` output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to [Delta Streaming: Ignoring Updates and Deletes](https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes).\n\nThe gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4510ca9b-fc33-4892-8e6a-34bea6f6c42c"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM grouped_count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4e5e243-9cad-46af-96f7-3bb149b39954"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Materialized View: Windowed Count of Hourly `gt` Events\n\nPlot the occurrence of all events grouped by `gt`.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Because we're using `complete` output mode for our gold table write, we cannot define a streaming plot on these files.\n\nInstead, we'll define a temp table based on the files written to our silver table. We will them use this table to execute our streaming queries.\n\nIn order to create a LIVE bar chart of the data, you'll need to fill out the <b>Plot Options</b> as shown:\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/ch5-plot-options.png\"/></div><br/>\n\n### Note on Gold Tables & Materialized Views\n\nWhen we call `display` on a streaming DataFrame or execute a SQL query on a streaming view, we are using memory as our sink. \n\nIn this case, we have already calculated all the values necessary to materialize our streaming view above in the gold table we've written to disk. \n\n**However**, we re-execute this logic on our silver table to generate streaming views, as structured streaming will not support reads from upstream files that have beem overwritten."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab11b66-c184-4d91-bd16-1749be6f425e"}}},{"cell_type":"code","source":["(spark.readStream\n  .format(\"delta\")\n  .load(activitySilverPath)\n  .createOrReplaceTempView(\"query_table\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca0003d6-294b-4e99-9047-4bd16244cd2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT gt, HOUR(Arrival_Time) hour, COUNT(*) total_events\nFROM query_table\nGROUP BY gt, HOUR(Arrival_Time)\nORDER BY hour"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12c0ff14-18bc-4d15-b8c6-765516cbfa91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Batch Load Data into Bronze Table\n\nWe can use the same pipeline to process batch data.\n\nBy loading our raw data into our bronze table, we will push it through our already running streaming logic.\n\nHere, we'll run 4 batches of around 170k records. We can track each batch through our streaming plots above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00cb7025-5e4d-424e-bcf5-07b54b77bbc9"}}},{"cell_type":"code","source":["for batch in range(4):\n  (spark\n    .read\n    .format(\"json\")\n    .schema(schema)\n    .load(\"/mnt/training/definitive-guide/data/activity-json/batch-{}\".format(batch))\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .save(activityBronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09f110f6-aa01-497c-ab2d-bb7b5bf56081"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that even on our small cluster, we can pass a batch of over 5 million records through our logic above without problems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efa117a7-4ce6-42e1-b77a-a26ec07da81f"}}},{"cell_type":"code","source":["(spark\n  .read\n  .format(\"json\")\n  .schema(schema)\n  .load(\"/mnt/training/definitive-guide/data/activity-json/batch\")\n  .write\n  .format(\"delta\")\n  .mode(\"append\")\n  .save(activityBronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1931ac5f-aede-4b4b-a17f-9bb2fc9c91b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> While our streaming materialized view above updates as data flows in, we can also easily generate this view from our `grouped_count` table. \n\nWe will need to re-run this query each time we wish to update the data. Run the below query now, and then after your batch has finished processing.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> The state reflected in a query on a registered Delta table will always reflect the most recent valid state of the files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24427459-4537-45c0-a5ef-c5de19766e4b"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM grouped_count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d38b764-9305-4b0e-a989-2b7692acace6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Wrapping Up\n\nFinally, make sure all streams are stopped."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10c49808-4768-4f6c-95fa-2888643af98c"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fc14fa8-b076-487d-8fce-4695d3e1c6f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Summary\n\nDelta Lake is ideally suited for use in streaming data lake contexts.\n\nUse the Delta Lake architecture to craft raw, query and summary tables to produce beautiful visualizations of key business metrics."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f88b9c64-374e-4e19-9444-2b2da25852de"}}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works.\n* <a href=\"http://lambda-architecture.net/#\" target=\"_blank\">Lambda Architecture</a>\n* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n* <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Reading structured streams from Kafka</a>\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n* <a href=\"https://docs.databricks.com/delta/delta-intro.html#case-study-multi-hop-pipelines#\" target=\"_blank\">Multi Hop Pipelines</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbfdc0fb-ca2d-430d-aca8-799cdd5fbbb2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.Delta-Architecture - Bronze,Silver, Gold","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1936480217922687}},"nbformat":4,"nbformat_minor":0}
